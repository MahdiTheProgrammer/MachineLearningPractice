# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EFKdn4TOuqeKxbgvQ0BD48yEwj20jlCo
"""

import torch
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
import random
# print(torch.__version__)

# """##Introduction to tensor
# ###Creating tensors
# """

# # scalar
# scalar = torch.tensor(7)
# scalar

# scalar.ndim

# # Get tensor back as Python int
# scalar.item()

# # Vector 
# vector = torch.tensor([7,7])
# vector

# vector.ndim

# vector.shape

# # MATRIX
# MATRIX = torch.tensor([[7,8],
#                       [9,10]])
# MATRIX

# MATRIX.ndim

# MATRIX[1]

# MATRIX.shape

# # TENSOR
# TENSOR = torch.tensor([[[1,2,3],
#                         [3,6,9],
#                         [2,4,5]],
#                        ])
# TENSOR

# TENSOR.ndim

# TENSOR.shape

# TENSOR[0]

# """### Random tensors

# Why random tensors?

# Random tensors are important because the way many neural networks learn is that they start with tensorts full of random numbers and then adjust those random numbers to better repesent the data.

# `Start with random numbers -> look at data -> update random numbers -> look at data -> update random numbers`
# """

# # Create a random tensor of size (3, 4)
# random_tensor = torch.rand(3,4)
# random_tensor

# # Create a random tensorr with similar shape to an image tensor
# random_image_size_tensor = torch.rand(size=(3,224,224)) # height, width, color channels (R , G , B)
# random_image_size_tensor.shape, random_image_size_tensor.ndim
# print(random_image_size_tensor)

# ## Zeros and ones

# # Create a tensor of all zeros 
# zeros = torch.zeros(size=(3,4))
# zeros

# zeros*random_tensor

# # My first training loop 
import torch 
from torch import nn 
import matplotlib.pyplot as plt

#Create *known* parameters
weight = 0.7
bias = 0.3

# Lets create some data
start = 0
end = 1
step = 0.02 
X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X + bias

X[:10], y[:10]

# Create train/test split 
train_split = int(0.8*len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]
len(X_train), len(y_train), len(X_test), len(y_test)

from prompt_toolkit.shortcuts.progress_bar.formatters import Label
def plot_predictions(train_data= X_train,
                     train_labels= y_train,
                     test_data = X_test, 
                     test_label = y_test,
                     predictions = None):
  
    plt.figure(figsize =(10,7))

    # Plot training data in blue 
    plt.scatter(train_data, train_labels, c='b', s=4, label="Training data")
  
    # Plot test data in green 
    plt.scatter(X_test, y_test, c="g", s=4, label="Test data")
  
    # Plot predictions if is not none 
    if predictions is not None:
        plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")
    
    # Set x and y-axis labels and title
    plt.xlabel("Input (x)")
    plt.ylabel("Output (y)")
    plt.title("Model predictions")
    
    # Show the legend 
    plt.legend(prop={"size": 14})
    
    # Display the plot
    plt.show()

plot_predictions()

# Create a Linear Regression model class 

class LinearRegressionModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1,
                                            dtype=torch.float),
                                requires_grad=True)
    
    self.bias = nn.Parameter(torch.randn(1,
                                         dtype=torch.float),
                             requires_grad=True)
    # Forward defines the computation in the model
  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.weights * x + self.bias 
model_0 = LinearRegressionModel()
model_0 = torch.load("save.pt")
model_0.eval()
with torch.inference_mode(): 
  y_preds = model_0(X_test)
plot_predictions(predictions = y_preds)

# Create the loss function 
torch.manual_seed(42)
loss_fn = nn.L1Loss()

# Create the optimzer 
optimizer = torch.optim.SGD(params=model_0.parameters(),
                            lr=0.00001)

# Set the number of epochs (how many times the model pass over the training data)
epochs = 100000

Loss_all = []
for epoch in range(epochs):
  ### Training 

  # Put model in training mode (this is the default state of a model)
  model_0.train()

  # 1. Forward pass on train data using the forward() method inside
  y_pred = model_0(X_train)

  # 2. Calculate the loss (how different are our models prediction to the ground truth)
  loss = loss_fn(y_pred, y_train)
  if epoch % 10000 == 0:
    print(f"loss: {loss}")
    print(model_0.state_dict())
  # 3. Zreo grad of the optimizer 
  optimizer.zero_grad()

  # 4. Loss backwards
  loss.backward()

  # 5. Progress the optimizer

  optimizer.step()
torch.save(model_0, 'save.pt')

with torch.inference_mode(): 
  y_preds = model_0(X_test)
  plot_predictions(predictions = y_preds)